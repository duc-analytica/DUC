{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "from acquire_prepare import acquire_oil\n",
    "from acquire_prepare import prep_data\n",
    "from explore import xgb_rank\n",
    "from model import get_scaled_df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sys # used in command to make entire array print by default\n",
    "\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# data manipulation \n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df,listofcolumns):\n",
    "    #   ''' arguments - (dataframe), columns to include in returned dataframe\n",
    "    #  ''' \n",
    "    newdf = df.copy()\n",
    "    col_list = df.columns \n",
    "    for column in col_list:\n",
    "        if column not in listofcolumns:\n",
    "            newdf.drop([column], axis=1, inplace=True)\n",
    "    return newdf\n",
    "\n",
    "def lregressiontest(df,xfeatures,yfeature,train_size):\n",
    "    y = df[yfeature]\n",
    "    X = filter_columns(df,xfeatures)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    column_names = X_train.columns\n",
    "    r_and_p_values = [pearsonr(X_train[col], y_train) for col in column_names]\n",
    "    corrdict = dict(zip(column_names, r_and_p_values))\n",
    "\n",
    "    ols_model = sm.OLS(y_train, X_train)\n",
    "    fit = ols_model.fit()\n",
    "    lm1 = LinearRegression(fit_intercept=False) \n",
    "    lm1.fit(X_train[xfeatures], y_train)\n",
    "    LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
    "         normalize=False)\n",
    "    lm1_y_intercept = lm1.intercept_\n",
    "    lm1_coefficients = lm1.coef_\n",
    "    y_pred_lm1 = lm1.predict(X_train[xfeatures])\n",
    "    mse = mean_squared_error(y_train, y_pred_lm1)\n",
    "    r2 = r2_score(y_train, y_pred_lm1)\n",
    "    return mse, r2, corrdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linreg(X_train, y_train):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    MAE = median_absolute_error(y_train, y_pred)\n",
    "    MSE = mean_squared_error(y_train, y_pred)\n",
    "    r2_train = r2_score(y_train, y_pred)\n",
    "    scatter = sns.scatterplot(x = y_train.recovery, y = y_pred.ravel())\n",
    "    reg_df = pd.DataFrame(lr.coef_, columns=X_vars)\n",
    "    reg_df = reg_df.transpose().sort_values(by=0, ascending=False)\n",
    "    reg_df.rename(index=str, columns={0: 'value'}, inplace=True)\n",
    "    abs_df = reg_df.copy()\n",
    "    abs_df['value'] = abs(abs_df['value'])\n",
    "    abs_df.sort_values(by='value', ascending=False)\n",
    "    return reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter\n",
    "\n",
    "def ridge(X_train, y_train):\n",
    "    rr = Ridge(alpha=.5)\n",
    "    rr.fit(X_train, y_train)\n",
    "    y_pred = rr.predict((X_train))\n",
    "    MAE = median_absolute_error(y_train, y_pred)\n",
    "    MSE = mean_squared_error(y_train, y_pred)\n",
    "    scatter = sns.scatterplot(x = y_train, y = y_pred.ravel())\n",
    "    return rr, rr.coef_, rr.intercept_, MAE, MSE, y_pred, scatter\n",
    "\n",
    "def svm(X_train, y_train):\n",
    "    svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "    svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "    svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,\n",
    "               coef0=1)\n",
    "    y_rbf = svr_rbf.fit(X_train, y_train).predict(X_train)\n",
    "    y_lin = svr_lin.fit(X_train, y_train).predict(X_train)\n",
    "    y_poly = svr_poly.fit(X_train, y_train).predict(X_train)\n",
    "    MSE_rbf = mean_squared_error(y_train, y_rbf)\n",
    "    MSE_lin = mean_squared_error(y_train, y_lin)\n",
    "    MSE_poly = mean_squared_error(y_train, y_poly)\n",
    "    return svr_rbf, svr_lin, svr_poly, MSE_rbf, MSE_lin, MSE_poly, y_rbf, y_lin, y_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire_oil()\n",
    "df = prep_data(df)\n",
    "df.reset_index(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['api14', 'sur_lat', 'sur_long', 'well_id']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe with Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get scaled data into scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = get_scaled_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the gross_perfs limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gross_perfs.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gross_perfs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_perfs_scaled_limit = (6000 - df.gross_perfs.min()) / (df.gross_perfs.max() - df.gross_perfs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_perfs_scaled_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = scaled_df.copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Feature Importance Comparing All Features')\n",
    "plt.show()\n",
    "\n",
    "# I think we can return xgb and model to then plot xgb.plot_importance(model,...)\n",
    "# Or maybe can save that line into a variable to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a viz of features ranked by gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get ranked features and that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features, best_df = xgb_rank(df, df.recovery)\n",
    "ranked_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['oil_gravity', 'tvd', 'vintage', 'gross_perfs', 'mid_point_lat', 'mid_point_long', 'formation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of integers to use for xticks\n",
    "c = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='feature', y='gain', data=best_df[:len(ranked_features)], palette=\"Blues_d\").tick_params(labelsize=15)\n",
    "\n",
    "group_labels = ranked_features\n",
    "\n",
    "# take the length of ranked_features and plot that many features\n",
    "plt.xticks(c[0:len(ranked_features)] , group_labels, weight='bold', fontsize=18)\n",
    "plt.yticks()\n",
    "\n",
    "plt.title('Feature Importance', weight='bold').set_fontsize('25')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('gain', weight='bold').set_fontsize('18')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the regression with all of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = scaled_df.columns\n",
    "X_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.recovery.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = scaled_df.columns\n",
    "\n",
    "# ['direction', 'frac_fluid_type', 'county', 'oper', 'formation',\n",
    "#  'lateral_class', 'proppant_ppf', 'frac_fluid_gpf', 'gross_perfs',\n",
    "#  'frac_stages', 'oil_gravity', 'gor_hist', 'tvd', 'mid_point_lat',\n",
    "#  'mid_point_long', 'vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = scaled_df[X_vars].copy()                                                    \n",
    "y = df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ranked by absolute value of coefficient:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter all features for only vertical wells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vert = prep_data(df,basin='all',direc='vertical',tvdmin=0,tvdmax=0,gperfmin=0,gperfmax=0,multiwell='all',clusterid=99)\n",
    "# df_vert_scaled = get_scaled_df(df)\n",
    "# df_vert_scaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "# These lines are to filter for only vertical wells:\n",
    "df_vert = scaled_df[scaled_df.scaled_direction == 1.0]\n",
    "df_vert.dropna(inplace=True)\n",
    "\n",
    "X = df_vert[X_vars].copy()                                                    \n",
    "y = df_vert[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ranked by absolute value of coefficient:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test only top features suggested by XGBoost, without filtering for vertical wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_proppant_ppf', 'scaled_gross_perfs', 'scaled_gor_hist', 'scaled_tvd', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_oper']\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = scaled_df[X_vars].copy()                                                    \n",
    "y = scaled_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test only top features suggested by XGBoost (exclude peak_boepd), WITH filtering for vertical wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_proppant_ppf', 'scaled_gross_perfs', 'scaled_gor_hist', 'scaled_tvd', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "df_vert = scaled_df[scaled_df.scaled_direction == 1.0]\n",
    "df_vert.dropna(inplace=True)\n",
    "\n",
    "X = df_vert[X_vars].copy()                                                    \n",
    "y = df_vert[['recovery']]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run XGBoost for each sub-basin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CP, Central Platform, including all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_df = scaled_df[(scaled_df['scaled_sub_basin'] == 0)]\n",
    "CP_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = CP_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, CP_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Feature Importance of Central Platform Sub-basin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = CP_df[X_vars].copy()                                                    \n",
    "y = CP_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delaware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_df = scaled_df[(scaled_df['scaled_sub_basin'] == 0.5)]\n",
    "Del_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Del_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Del_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Feature Importance of Delaware Sub-basin')\n",
    "plt.show()\n",
    "\n",
    "# I think we can return xgb and model to then plot xgb.plot_importance(model,...)\n",
    "# Or maybe can save that line into a variable to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Del_df[X_vars].copy()                                                    \n",
    "y = Del_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1)]\n",
    "Mid_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Mid_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Mid_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Feature Importance of Midland Sub-basin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended features for XGB:\n",
    "\n",
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', 'scaled_frac_fluid_gpf',\n",
    "              'scaled_tvd', 'scaled_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_df[X_vars].copy()                                                    \n",
    "y = Mid_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended features:\n",
    "\n",
    "XGB X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', 'scaled_frac_fluid_gpf',\n",
    "              'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "\n",
    "LM X_vars = ['scaled_gross_perfs', 'scaled_oil_gravity', 'scaled_tvd', 'scaled_vintage', \n",
    "             'scaled_direction', 'scaled_formation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midland, horizontal wells only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.scaled_direction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_Hor_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0)]\n",
    "Mid_Hor_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Mid_Hor_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Mid_Hor_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Midland, Horizontal Wells Only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars].copy()                                                    \n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midland, vertical wells only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_Ver_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 1.0)]\n",
    "Mid_Ver_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Mid_Ver_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Mid_Ver_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Midland, Vertical Wells Only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Ver_df[X_vars].copy()                                                    \n",
    "y = Mid_Ver_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midland, Horizontal Wells, Gross Perforations < 6000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_Hor_Low_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0) & (scaled_df['scaled_gross_perfs'] < gross_perfs_scaled_limit)]\n",
    "Mid_Hor_Low_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Mid_Hor_Low_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Mid_Hor_Low_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Midland, Horizontal Wells, Gross Perforations < 6000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_Low_df[X_vars].copy()                                                    \n",
    "y = Mid_Hor_Low_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midland, Horizontal Wells, Gross Perforations > 6000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0) & (scaled_df['scaled_gross_perfs'] < gross_perfs_scaled_limit)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0) & (scaled_df['scaled_gross_perfs'] > gross_perfs_scaled_limit)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_Hor_High_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0) & (scaled_df['scaled_gross_perfs'] > gross_perfs_scaled_limit)]\n",
    "Mid_Hor_High_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df = Mid_Hor_High_df.drop(columns=['recovery']).copy()\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'seed' : 493\n",
    "}\n",
    "dtrain = xgb.DMatrix(xgb_df, Mid_Hor_High_df.recovery, feature_names=xgb_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.title('Midland, Horizontal Wells, Gross Perforations > 6000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "          'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "          'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "          'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "          'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_High_df[X_vars].copy()                                                    \n",
    "y = Mid_Hor_High_df[target]\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "reg_df, abs_df, lr, lr.coef_, lr.intercept_, MAE, MSE, r2_train, y_pred, scatter = linreg(X_train, y_train)\n",
    "print('lr.intercept:', lr.intercept_[0])\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('r2_train:', r2_train)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abs_df:')\n",
    "abs_df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Harness and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First try on Midland sub-basin, horizontal wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mid_Hor_df = scaled_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0)]\n",
    "Mid_Hor_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended features:\n",
    "\n",
    "XGB X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', 'scaled_frac_fluid_gpf',\n",
    "              'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "\n",
    "LM X_vars = ['scaled_gross_perfs', 'scaled_oil_gravity', 'scaled_tvd', 'scaled_vintage', \n",
    "             'scaled_direction', 'scaled_formation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()\n",
    "\n",
    "# R² is a statistical measure of how close the data are to the fitted \n",
    "# regression line. \n",
    "# R² is the percentage of the response variable variation that is \n",
    "# explained by a linear model. Or:\n",
    "# R² = Explained variation / Total variation\n",
    "# R² is always between 0 and 100%:\n",
    "# 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "# 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "# In general, the higher the R-squared, the better the model fits your data. \n",
    "\n",
    "# Root Mean Square Error (RMSE) is the standard deviation of the \n",
    "# residuals (prediction errors), a measure of how spread out these \n",
    "# residuals are. In other words, it tells you how concentrated the data \n",
    "# is around the line of best fit.\n",
    "\n",
    "# The mean squared error (MSE) of an estimator measures the average of \n",
    "# the squares of the errors—that is, the average squared difference \n",
    "# between the estimated values and what is estimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_gross_perfs', 'scaled_oil_gravity', 'scaled_tvd', \n",
    "          'scaled_vintage', 'scaled_direction', 'scaled_formation']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(kernel='rbf', gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(kernel='linear', gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poly kernel is best, but r2 is only .055."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(kernel='poly', gamma='scale', C=1.0, epsilon=0.2, degree=25)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using features from linear regression model is even better, but still only r2 = .057."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_gross_perfs', 'scaled_oil_gravity', 'scaled_tvd', \n",
    "          'scaled_vintage', 'scaled_direction', 'scaled_formation']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(kernel='poly', gamma='scale', C=1.0, epsilon=0.2, verbose=True)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried \"cheating\" by adding mid_point_long and mid_point_lat, but still r2 = .06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int64', 'float64', 'float']\n",
    "\n",
    "temp_df = df.select_dtypes(include=numerics)\n",
    "temp_df = temp_df.drop(columns=['api14', 'proppant_ppf', 'frac_fluid_gpf', \n",
    "        'gross_perfs', 'frac_stages', 'oil_gravity', 'peak_boepd', 'oil_hist', \n",
    "        'gas_hist', 'gor_hist', 'ip90_boeqpd', 'tvd', 'sur_lat', 'sur_long', \n",
    "        'well_id', 'mid_point_lat', 'mid_point_long', 'recovery_per_foot', \n",
    "        'months_active', 'recovery_per_month', 'vintage', 'vintage_bin', \n",
    "        'encoded_direction', 'encoded_frac_fluid_type', 'encoded_county', \n",
    "        'encoded_oper', 'encoded_formation', 'encoded_sub_basin', 'encoded_lateral_class', \n",
    "        'scaled_county'])\n",
    "temp_Mid_Hor_df = temp_df[(scaled_df['scaled_sub_basin'] == 1) & (scaled_df['scaled_direction'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_vars = ['scaled_direction', 'scaled_frac_fluid_type', 'scaled_oper', \n",
    "#           'scaled_formation', 'scaled_sub_basin', 'scaled_lateral_class', \n",
    "#           'scaled_proppant_ppf', 'scaled_frac_fluid_gpf', 'scaled_gross_perfs', \n",
    "#           'scaled_frac_stages', 'scaled_oil_gravity', 'scaled_gor_hist', \n",
    "#           'scaled_tvd', 'scaled_vintage']\n",
    "\n",
    "X_vars = ['scaled_gross_perfs', 'scaled_oil_gravity', 'scaled_tvd', \n",
    "          'scaled_vintage', 'scaled_direction', 'scaled_formation',\n",
    "          'scaled_mid_point_lat', 'scaled_mid_point_long']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = temp_Mid_Hor_df[X_vars]\n",
    "y = temp_Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(kernel='poly', gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# #############################################################################\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,\n",
    "               coef0=1)\n",
    "\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "lw = 2\n",
    "\n",
    "svrs = [svr_rbf, svr_lin, svr_poly]\n",
    "kernel_label = ['RBF', 'Linear', 'Polynomial']\n",
    "model_color = ['m', 'c', 'g']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
    "for ix, svr in enumerate(svrs):\n",
    "    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw,\n",
    "                  label='{} model'.format(kernel_label[ix]))\n",
    "    axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor=\"none\",\n",
    "                     edgecolor=model_color[ix], s=50,\n",
    "                     label='{} support vectors'.format(kernel_label[ix]))\n",
    "    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
    "                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
    "                     facecolor=\"none\", edgecolor=\"k\", s=50,\n",
    "                     label='other training data')\n",
    "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
    "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
    "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #############################################################################\n",
    "# X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "#           'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "X_vars = ['scaled_proppant_ppf']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=.2)\n",
    "svr_lin = SVR(kernel='linear', C=1.0, gamma='scale')\n",
    "svr_poly = SVR(kernel='poly', C=1.0, gamma='scale', degree=3, epsilon=.2,\n",
    "               coef0=1)\n",
    "\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "lw = 2\n",
    "\n",
    "svrs = [svr_rbf, svr_lin, svr_poly]\n",
    "kernel_label = ['RBF', 'Linear', 'Polynomial']\n",
    "model_color = ['m', 'c', 'g']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
    "for ix, svr in enumerate(svrs):\n",
    "    print(ix)\n",
    "    print(svr)\n",
    "    print()\n",
    "    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw,\n",
    "                  label='{} model'.format(kernel_label[ix]))\n",
    "#     axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor=\"none\",\n",
    "#                      edgecolor=model_color[ix], s=50,\n",
    "#                      label='{} support vectors'.format(kernel_label[ix]))\n",
    "#     axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
    "#                      y[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
    "#                      facecolor=\"none\", edgecolor=\"k\", s=50,\n",
    "#                      label='other training data')\n",
    "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                        ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
    "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
    "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# This is the code that worked above, but only on one at a time.\n",
    "# m = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "# m_fit = m.fit(X, y) \n",
    "# y_pred = m_fit.predict(X)\n",
    "# plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# ax = plt.subplot(1, 1, 1)\n",
    "# ax.plot(y_pred, y, '.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From curriculum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_XGB_df = Mid_Hor_df[['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', 'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']]\n",
    "SVR_XGB_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "m = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "m_fit = m.fit(X, y) \n",
    "y_pred = m_fit.predict(X)\n",
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y)\n",
    "# root mean squared error\n",
    "rmse = math.sqrt(mse)\n",
    "# r^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "print('r2: ', r2, '   rmse:', rmse, '   mse:', mse)\n",
    "plt.plot(y_pred, 'r-', y, 'b-')\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables:\n",
    "X_vars = ['scaled_proppant_ppf', 'scaled_gor_hist', 'scaled_gross_perfs', \n",
    "          'scaled_frac_fluid_gpf', 'scaled_tvd', 'scaled_oper']\n",
    "\n",
    "# Dependent (target) variable, it is continuous\n",
    "target = ['recovery']\n",
    "\n",
    "X = Mid_Hor_df[X_vars]\n",
    "y = Mid_Hor_df[target]\n",
    "\n",
    "\n",
    "# random_state=0 tells the sklearn function that you are not setting a seed\n",
    "# if you want to be able to duplicate results, set random_state to a positive integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "\n",
    "train = np.array(pd.concat([X_train, y_train], axis=1))\n",
    "test = np.array(pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Support Vector Regressor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SVR(kernel='rbf', C=1, gamma=0.1)\n",
    "# C is our regularization constant\n",
    "m_fit = m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict in-sample/training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m_fit.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y_train)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "import math\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, 'b-', y_train, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the predicted values with the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(y_pred, y_train, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on out-of-sample/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "import math\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, 'b-', y_test, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to add filter_columns for the following to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and select algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df,listofcolumns):\n",
    "    #   ''' arguments - (dataframe), columns to include in returned dataframe\n",
    "    #  ''' \n",
    "    newdf = df.copy()\n",
    "    col_list = df.columns \n",
    "    for column in col_list:\n",
    "        if column not in listofcolumns:\n",
    "            newdf.drop([column], axis=1, inplace=True)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lregressiontest(df,xfeatures,yfeature,train_size):\n",
    "    y = df[yfeature]\n",
    "    X = filter_columns(df,xfeatures)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=123)\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    column_names = X_train.columns\n",
    "    r_and_p_values = [pearsonr(X_train[col], y_train) for col in column_names]\n",
    "    corrdict = dict(zip(column_names, r_and_p_values))\n",
    "\n",
    "    ols_model = sm.OLS(y_train, X_train)\n",
    "    fit = ols_model.fit()\n",
    "    lm1 = LinearRegression(fit_intercept=False) \n",
    "    lm1.fit(X_train[xfeatures], y_train)\n",
    "    LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
    "         normalize=False)\n",
    "    lm1_y_intercept = lm1.intercept_\n",
    "    lm1_coefficients = lm1.coef_\n",
    "    y_pred_lm1 = lm1.predict(X_train[xfeatures])\n",
    "    mse = mean_squared_error(y_train, y_pred_lm1)\n",
    "    r2 = r2_score(y_train, y_pred_lm1)\n",
    "    return mse, r2, corrdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['proppant_ppf', 'gor_hist', 'frac_fluid_gpf', \n",
    "          'months_active', 'lateral_len', 'ip90_boeqpd', 'county']\n",
    "\n",
    "lregressiontest(scaled_df, X_vars, 'recovery', .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
